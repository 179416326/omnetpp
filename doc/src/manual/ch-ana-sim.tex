\chapter{Result Recording and Analysis}
\label{cha:analyzing-simulation-results}

\section{Result Recording}

{\opp} provides built-in support for recording simulation results, via
\textit{output vectors} and \textit{output scalars}. Output vectors are
time series data, recorded from simple modules or channels. You can use
output vectors to record end-to-end delays or round trip times of packets,
queue lengths, queueing times, module state, link utilization, packet
drops, etc. -- anything that is useful to get a full picture of what
happened in the model during the simulation run.

Output scalars are summary results, computed during the simulation and
written out when the simulation completes. A scalar result may be an
(integer or real) number, or may be a statistical summary comprised of
several fields such as count, mean, standard deviation, sum, minimum,
maximum, etc., and optionally histogram data.

Results may be collected and recorded in two ways:

\begin{enumerate}
  \item Based on the signal mechanism, using declared statistics;
  \item Directly from C++ code, using the simulation library
\end{enumerate}

The second method has been the traditional way of recording results. The
first method, based on signals and declared statistics, was introduced in
{\opp} 4.1, and it is preferable because it allows you to always record the
results in the form you need, without requiring heavy instrumentation or
continuous tweaking of the simulation model.

\subsection{Using Signals and Declared Statistics}

This approach combines the signal mechanism (see
\ref{sec:simple-modules:signals}) and NED properties (see
\ref{sec:ch-ned-lang:properties}) in order to de-couple the generation of
results from their recording, thereby providing more flexibility in what to
record and in which form. The details of the solution have been described
in section \ref{sec:ch-simple-modules:statistic-signals} in detail; here we
just give a short overview.

Statistics are declared in the NED files with the \ttt{@statistic} property,
and modules emit values using the signal mechanism. The simulation framework
records data by adding special result file writer listeners to the signals.
By being able to choose what listeners to add, the user can control what to
record in the result files and what computations to apply before recording.
The aforementioned section \ref{sec:ch-simple-modules:statistic-signals}
also explains how to instrument simple modules and channels for signals-based
result recording.

The signals approach allows for calculation of aggregate statistics (such as the
total number of packet drops in the network) and for implementing a warm-up
period without support from module code. It also allows you to write
dedicated statistics collection modules for the simulation, also without
touching existing modules.

The same configuration options that were used to control result recording
with \cclass{cOutVector} and \ffunc{recordScalar()} also work when utilizing
the signals approach, and there are extra configuration options to make
the additional possibilities accessible.

\subsection{Direct Result Recording}

With this approach, scalar and statistics results are collected in class
variables inside modules, then recorded in the finalization phase via
\ffunc{recordScalar()} calls. Vectors are recorded using
\cclass{cOutVector} objects. To record more details, like the
minimum/maximum value or the standard deviation, \cclass{cStdDev} and
\cclass{cWeightedStdDev} can be used, and for recording the distribution
there are histogram and other distribution estimation classes
(\cclass{cDoubleHistogram}, \cclass{cLongHistogram}, \cclass{cPSquare},
\cclass{cKSplit}, and others). These classes are described in sections
\ref{sec:ch-sim-lib:statistics} and \ref{sec:ch-sim-lib:result-recording}.
Recording of individual vectors, scalars and statistics can be enabled or
disabled via the configuration (ini file), and it is also the place to set
up recording intervals for vectors.

The drawback of recording results directly from modules is that result
recording is hardcoded in modules, and even simple requirement changes
(e.g. record the average delay instead of each delay value, or vice versa)
requires either code change or an excessive amount of result collection
code in the modules.



\section{Configuring Result Collection}
\label{sec:ana-sim:config-results}

\subsection{Configuring Signal-Based Statistics Recording}
\label{sec:ana-sim:signal-based-recording}

Signal-based statistics recording has been designed so that it can be
easily configured to record a ``default minimal'' set of results, a
``detailed'' set of results, and a custom set of results (by modifying
the previous ones, or defined from scratch).

Recording can be tuned with the \fconfig{result-recording-modes}
per-object configuration option. The ``object'' here is the statistic,
which is identified by the full path (hierarchical name) of the module or
connection channel object in question, plus the name of the statistic
(which is the ``index'' of \fprop{@statistic} property, i.e. the name in
the square brackets). Thus, configuration keys have the syntax
\textit{<module-full-path>.<statistic-name>.}\ttt{result-recording-modes=}.

The \fconfig{result-recording-modes} option accepts one or more items as value,
separated by comma. An item may be a result recording mode (surprise!), and
two words with a special meaning, \ttt{default} and \ttt{all}:

\begin{itemize}
\item A \textit{result recording mode} means any item that may occur in the
      \ttt{record} key of the \fprop{@statistic} property; for example,
      \ttt{count}, \ttt{sum}, \ttt{mean}, \ttt{vector((count-1)/2)}.
\item \tbf{\ttt{default}} stands for the set of non-optional items from
      the \fprop{@statistic} property's \ttt{record} list, that is, those
      without question marks.
\item \tbf{\ttt{all}} means all items from the \fprop{@statistic} property's
      \ttt{record} list, including the ones with question marks.
\end{itemize}

The default value is \ttt{default}.

A lone ``-'' as option value disables all recording modes.

\textit{Recording mode} items in the list may be prefixed with ``+'' or
``-'' to add/remove them from the set of result recording modes. The
initial set of result recording modes is \ttt{default}; if the first item
is prefixed with ``+'' or ``-'', then that and all subsequent items are
understood as modifying the set; if the first item does not start with with
``+'' or ``-'', then it replaces the set, and further items are understood
as modifying the set.

This sounds more complicated than it is; an example will make it clear.
Suppose we are configuring the following statistic:

\begin{ned}
@statistic[foo](record=count,mean,max?,vector?);
\end{ned}

With the following the ini file lines (see results in comments):

\begin{inifile}
**.result-recording-modes = default  # --> count, mean
**.result-recording-modes = all      # --> count, mean, max
**.result-recording-modes = -        # --> none
**.result-recording-modes = mean     # --> only mean (disables 'default')
**.result-recording-modes = default,-vector,+histogram # --> count,mean,histogram
**.result-recording-modes = -vector,+histogram      # --> same as above
**.result-recording-modes = all,-vector,+histogram  # --> count,mean,max,histogram
\end{inifile}

Here is another example which shows how to write a more specific option
key. The following line applies to \ttt{queueLength} statistics of
\ttt{fifo[]} submodule vectors anywhere in the network:

\begin{inifile}
**.fifo[*].queueLength.result-recording-modes = +vector  # default modes plus vector
\end{inifile}

In the result file, the recorded scalars will be suffixed with the recording mode,
i.e. the mean of \ttt{queueingTime} will be recorded as \ttt{queueingTime:mean}.

\begin{note}
Signal-based statistics recording forms a layer above the normal scalar and
vector recording infrastructure, so options like \fconfig{scalar-recording},
\fconfig{vector-recording}, \fconfig{vector-recording-intervals} also affect it.
These options are described in the following sections.
\end{note}


\subsection{Warm-up Period}
\label{sec:ana-sim:warmup-period}

The \fconfig{warmup-period} option specifies the length of the initial
warm-up period. When set, results belonging to the first $x$ seconds
of the simulation will not be recorded into output vectors, and will
not be counted into the calculation of output scalars.
This option is useful for steady-state simulations. The default is 0s
(no warmup period).

Example:

\begin{inifile}
warmup-period = 20s
\end{inifile}

Results recorded via signal-based statistics automatically obey the warm-up
period setting, but modules that compute and record scalar results
manually (via \ffunc{recordScalar()}) need to be modified so that they take
the warm-up period into account.

\begin{note}
When configuring a warm-up period, make sure that modules that compute and
record scalar results manually via \ffunc{recordScalar()} actually obey the
warm-up period in the C++ code.
\end{note}

The warm-up period is available via the \ffunc{getWarmupPeriod()} method of
the \ttt{simulation} object, so the C++ code that updates the corresponding
state variables needs to be surrounded with an \textit{if} statement:

Old:

\begin{cpp}
dropCount++;
\end{cpp}

New:

\begin{cpp}
if (simTime() >= simulation.getWarmupPeriod())
    dropCount++;
\end{cpp}


\subsection{Result File Names}

Simulation results are recorded into \textit{output scalar files} that
actually hold statistics results as well, and \textit{output vector
files}. The usual file extension for scalar files is \ttt{.sca}, and
for vector files \ttt{.vec}.

Every simulation run generates a single scalar file and a vector file.
The file names can be controlled with the \fconfig{output-vector-file}
and \fconfig{output-scalar-file} options. These options rarely need
to be used, because the default values are usually fine. The defaults
are:

\begin{inifile}
output-vector-file = "${resultdir}/${configname}-${runnumber}.vec"
output-scalar-file = "${resultdir}/${configname}-${runnumber}.sca"
\end{inifile}

Here, \ttt{\$\{resultdir\}} is the value of the \fconfig{result-dir}
configuration option which defaults to \ttt{results/}, and
\ttt{\$\{configname\}} and \ttt{\$\{runnumber\}} are the name of
the configuration name in the ini file (e.g. \ttt{[Config PureAloha]}),
and the run number. Thus, the above defaults generate file names
like \ttt{results/PureAloha-0.vec}, \ttt{results/PureAloha-1.vec},
and so on.

\begin{note}
  In {\opp} 3.x, the default result file names were \ttt{omnetpp.vec} and
  \ttt{omnetpp.sca}, and scalar files were always appended to, rather than
  being overwritten as in the 4.x version. When needed, the old behavior
  for scalar files can be turned back on by setting
  \ttt{output-scalar-file-append=true} in the configuration.
\end{note}


\subsection{Configuring Scalar Results}
\label{sec:ana-sim:scalar-results}

Recording results into the scalar file can be turned off globally by adding
the following line to the ini file. Any existing file with the same name
will still be removed before the simulation starts.

\begin{inifile}
**.scalar-recording = false
\end{inifile}

Recording scalar results can be enabled or disabled individually, using
patterns. The syntax of the configuration option is
\textit{<module-full-path>.<scalar-name>.}\ttt{scalar-recording=}\textit{true/false},
where both \textit{<module-full-path>} and \textit{<scalar-name>}
may contain wildcards (see \ref{sec:ch-config-sim:wildcards}).
\textit{<scalar-name>} is the signal name, or the string passed to the
\ffunc{recordScalar()} call. By default, the recording of all scalars is
enabled.

The following example turns off recording all scalar results, except
end-to-end delays and those produced by TCP modules:

\begin{inifile}
**.tcp.**.scalar-recording = true
**.endToEndDelay.scalar-recording = true
**.scalar-recording = false
\end{inifile}

%% TODO **.eed:histogram.scalar-recording=false    disables histogram


\subsection{Configuring Output Vectors}
\label{sec:ana-sim:vector-config}

Recording output vector results can be turned off globally by adding
the following line to the ini file. Any existing file with the same name
will still be removed before the simulation starts.

\begin{inifile}
**.vector-recording = false
\end{inifile}

The size of output vector files can easily reach the magnitude of several
hundred megabytes, but very often, only some of the recorded statistics are
interesting to the analyst. {\opp} allows you to control which vectors you
want to record, and to specify one or more collection intervals.

Output vectors can be configured with the \fconfig{vector-recording} and
\fconfig{vector-recording-intervals} per-object options. The syntax of the
configuration options are
\textit{<module-full-path>.<vector-name>.}\ttt{vector-recording=}\textit{true/false},
%line is outside the margins
and \textit{<module-full-path>.<vector-name>.}\ttt{vector-recording-intervals=}\textit{<intervals>},
where both \textit{<module-full-path>} and \textit{<vector-name>} may
contain wildcards (see \ref{sec:ch-config-sim:wildcards}).
\textit{<vector-name>} is the signal name, or the name string of the
\ffunc{cOutVector} object. By default, all output vectors are turned
on for the whole duration the simulation.

The following example only records the \ttt{queueLength} vectors and
\ttt{endToEndDelay} in \ttt{voiceApp} modules, and turns off the rest:

\begin{inifile}
**.queueLength.vector-recording = true
**.voiceApp.endToEndDelay.vector-recording = true
**.vector-recording = false
\end{inifile}

For the \fconfig{vector-recording-intervals} option, one can specify
one or more intervals in the \textit{<startTime>..<stopTime>} syntax,
separated by comma. \textit{<startTime>} or \textit{<stopTime>} need
to be given with measurement units, and both can be omitted to denote
the beginning and the end of the simulation, respectively.

The following example limits all vectors to three intervals, except
\ttt{dropCount} vectors which will be recorded during the whole
simulation run:

\begin{inifile}
**.dropCount.vector-recording-intervals = 0..
**.vector-recording-intervals = 0..1000s, 5000s..6000s, 9000s..
\end{inifile}

A third per-vector configuration option is \fconfig{vector-record-eventnumbers},
which specifies whether to record event numbers for an output vector.
(Simulation time and value are always recorded. Event numbers are needed
by the Sequence Chart Tool, for example.) Event number recording is enabled
by default; it may be turned off to save disk space.

\begin{inifile}
**.vector-record-eventnumbers = false
\end{inifile}

If the (default) \cclass{cIndexedFileOutputVectorManager} class is used to
record output vectors, there are two more options to fine-tune its resource
usage. \ttt{output-vectors-memory-limit} specifies the total memory that
can be used for buffering output vectors. Larger values produce less
fragmented vector files (i.e. cause vector data to be grouped into larger
chunks), and therefore allow more efficient processing later.
\ttt{vector-max-buffered-values} specifies the maximum number of values to
buffer per vector, before writing out a block into the output vector file.
The default is no per-vector limit (i.e. only the total memory limit is in
effect.)


\subsection{Saving Parameters as Scalars}

When you are running several simulations with different parameter
settings, you'll usually want to refer to selected
input parameters in the result analysis as well -- for example when
drawing a throughput (or response time) versus load (or network
background traffic) plot. Average throughput or response time numbers
are saved into the output scalar files, and it is useful for the input
parameters to get saved into the same file as well.

For convenience, {\opp} automatically saves the iteration variables
into the output scalar file if they have numeric value, so they can
be referred to during result analysis.

\begin{warning}
    If an iteration variable has non-numeric value, it will not be recorded
    automatically and cannot be used during analysis. This can happen
    unintentionally if you specify units inside an iteration variable list:
\begin{inifile}
**.param = exponential( ${mean=0.2s, 0.4s, 0.6s} )  #WRONG!
**.param = exponential( ${mean=0.2, 0.4, 0.6}s )    #OK
\end{inifile}
\end{warning}

Module parameters can also be saved, but this has to be
requested by the user, by configuring \ttt{param-record-as-scalar=true} for the
parameters in question. The configuration key is a pattern that
identifies the parameter, plus \ttt{.param-record-as-scalar}. An example:

\begin{inifile}
**.host[*].networkLoad.param-record-as-scalar = true
\end{inifile}

This looks simple enough, however there are three pitfalls:
non-numeric parameters, too many matching parameters, and
random-valued volatile parameters.

First, the scalar file only holds numeric results, so non-numeric
parameters cannot be recorded -- that will result in a runtime
error.

Second, if wildcards in the pattern match too many parameters, that
might unnecessarily increase the size of the scalar file. For example,
if the \ttt{host[]} module vector size is 1000 in the example below, then the
same value (3) will be saved 1000 times into the scalar file, once for
each host.

\begin{inifile}
**.host[*].startTime = 3
**.host[*].startTime.param-record-as-scalar = true  # saves "3" once for each host
\end{inifile}

Third, recording a random-valued volatile parameter will just save a
random number from that distribution. This is rarely what you need, and
the simulation kernel will also issue a warning if this happens.

\begin{inifile}
**.interarrivalTime = exponential(1s)
**.interarrivalTime.param-record-as-scalar = true  # wrong: saves random values!
\end{inifile}

These pitfalls are quite common in practice, so it is usually better
to rely on the iteration variables in the result analysis.
That is, one can rewrite the above example as

\begin{inifile}
**.interarrivalTime = exponential( ${mean=1}s )
\end{inifile}

and refer to the \ttt{\$mean} iteration variable instead of the
interarrivalTime module parameter(s) during result analysis.
\ttt{param-record-as-scalar=true} is not needed, because iteration variables are
automatically saved into the result files.


\subsection{Recording Precision}
\label{sec:outputfile-precision}

Output scalar and output vector files are text files, and floating point
values (\ttt{double}s) are recorded into it using \ttt{fprintf()}'s
\ttt{"\%g"} format. The number of significant digits can be configured
using the \fconfig{output-scalar-precision} and \fconfig{output-vector-precision}
configuration options.

The default precision is 12 digits. The following has to be considered when
setting a different value:

IEEE-754 doubles are 64-bit numbers. The mantissa is 52 bits, which is
roughly equivalent to 16 decimal places (52*log(2)/log(10)). However, due
to rounding errors, usually only 12..14 digits are correct, and the rest is
pretty much random garbage which should be ignored. However, when you
convert the decimal representation back into a \ttt{double} for result
processing, an additional small error will occur, because 0.1, 0.01, etc.
cannot be accurately represented in binary. This conversion error is
usually smaller than what that the \ttt{double} variable already had
before recording into the file. However, if it is important, you can
eliminate this error by setting the recording precision to 16 digits or
more (but again, be aware that the last digits are garbage). The practical
upper limit is 17 digits, setting it higher doesn't make any difference in
\ttt{fprintf()}'s output.

% To see finite machine precision and rounding errors, try this code:
%
% \ begin{verbatim}
% double x = 0.1;
% while (true)  {
%    printf("%.15g\n", x);
%    x = x + 0.1;
% }
% \ end{verbatim}
%
% The following, more advanced version will also print the error of
% converting back from text to double:
%
% \ begin{verbatim}
% double x = 0.1;
% while (true) {
%     char line[120];
%     sprintf(line, "%.15g \t%.14g \t%.13g \t%.12g", x, x, x, x);
%     double x15, x14, x13, x12;
%     sscanf(line, "%lg%lg%lg%lg", &x15, &x14, &x13, &x12);
%     printf("%s \t| %g  %g  %g  %g\n", line, (x15-x), (x14-x), (x13-x), (x12-x));
%     x = x + 0.1;
% }
% \ end{verbatim}
%    s
% For the complexity of the issue, see "What Every Computer Scientist
% Should Know About Floating-Point Arithmetic" by David Goldberg.

Errors resulting from converting to/from decimal representation can be
eliminated by choosing an output vector/output scalar manager class
which stores \ttt{double}s in their native binary form.
The appropriate configuration options are \fconfig{outputvectormanager-class}
and \fconfig{outputvectormanager-class}. For example,
\cclass{cMySQLOutputScalarManager} and \cclass{cMySQLOutputScalarManager}
provided in \ttt{samples/database} fulfill this requirement.

However, before worrying too much about rounding and conversion errors,
consider the \textit{real} accuracy of your results:

\begin{itemize}
  \item{in real life, it is very difficult to measure quantities (weight, distance,
     even time) with more than a few digits of precision. What precision
     are your input data? For example, if you approximate inter-arrival
     time as \textit{exponential(0.153)} when the mean is really
     \textit{0.152601...} and the distribution is not even exactly exponential,
     you are already starting out with a bigger error than rounding can cause.}

  \item{the simulation model is itself an approximation of real life. How much
     error do the (known and unknown) simplifications cause in the results?}
\end{itemize}

%% TODO also hint that results can be directed to database etc! by changing the implementation that cEnvir methods delegate to. (list cEnvir methods!)


\section{Overview of the Result File Formats}

Both output vector and scalar files are textual, line-oriented files.
The advantage of a text-based format is that it is very accessible
with a wide range of tools and languages. The format of result files is
documented in detail in Appendix \ref{cha:result-file-formats}.

%XXX move away:
%  \footnote{Recording is actually configurable, and one can record
%  results into a database as well, by writing appropriate result
%  manager classes and activating them in the configuration.}

By default, each file contains data from one run only.

Result files start with a header that contains several attributes of the
simulation run: a reasonably globally unique run ID, the network NED type
name, the experiment-measurement-replication labels, the values of
iteration variables and the repetition counter, the date and time, the host
name, the process id of the simulation, random number seeds, configuration
options, and so on. These data can be useful during result processing, and
increase the reproducibility of the results.

%%FIXME example header!

Vectors are recorded into a separate file for practical reasons: vector
data usually consume several magnitudes more disk space than scalars.

\subsection{Output Vector Files}

All output vectors from a simulation run are recorded into the same file.
The following sections describe the format of the file, and
how to process it.

An example file fragment (without header):

\begin{filelisting}
...
vector 1   net.host[12]  responseTime  TV
1  12.895  2355.66
1  14.126  4577.66664666
vector 2   net.router[9].ppp[0] queueLength  TV
2  16.960  2
1  23.086  2355.66666666
2  24.026  8
...
\end{filelisting}

There two types of lines: vector declaration lines (beginning with the word
\ttt{vector}), and data lines. A \textit{vector declaration line}
introduces a new output vector, and its columns are: vector Id, module of
creation, name of \cclass{cOutVector} object, and multiplicity (usually 1).
Actual data recorded in this vector are on \textit{data lines} which begin
with the vector Id. Further columns on data lines are the simulation time
and the recorded value.

% FIXME plus attribute lines!!! also event numbers

Since {\opp} 4.0, vector data are recorded into the file clustered by
output vectors, which, combined with index files, allows much more
efficient processing. Using the index file, tools can extract particular
vectors by reading only those parts of the file where the desired data are
located, and do not need to scan through the whole file linearly.


\subsection{Scalar Result Files}

Fragment of an output scalar file (without header):

\begin{filelisting}
...
scalar "lan.hostA.mac" "frames sent"  99
scalar "lan.hostA.mac" "frames rcvd"  3088
scalar "lan.hostA.mac" "bytes sent"   64869
scalar "lan.hostA.mac" "bytes rcvd"   3529448
...
\end{filelisting}

Every scalar generates one \ttt{scalar} line in the file.

Statistics objects (\cclass{cStatictic} subclasses such as \cclass{cStdDev})
generate several lines: mean, standard deviation, etc.

%% FIXME TODO attributes, statistics example, etc

\section{The Analysis Tool in the Simulation IDE}

The Simulation IDE provides an Analysis Tool for analysis and visualization
of simulation results. The Analysis Tool lets you load several result files
at once, and presents their contents somewhat like a database. You can
browse the results, select the particular data you are interested in
(scalars, vectors, histograms), apply processing steps, and create various
charts or plots from them. Data selection, processing and charting steps
can be freely combined, resulting in a high degree of freedom.
These steps are grouped into and stored as "recipes", which get automatically
re-applied when new result files are added or existing files are
replaced. This automation spares the user lots of repetitive manual work,
without resorting to scripting.

The Analysis Tool is covered in detail in the User Guide.


\section{Scave Tool}
\index{scavetool}\label{sec:ana-sim:scavetool}

Much of the IDE Analysis Tool's functionality is available on the command
line as well, via the \fprog{scavetool} program. \fprog{scavetool} is
suitable for filtering and basic processing of result files, and
exporting the result in various formats digestible for other tools.
\fprog{scavetool} has no graphics capabilities, but it can be used
to produce files that can be directly plotted with other tools like
gnuplot (see \ref{sec:ana-sim:gnuplot}).

When \fprog{scavetool} is invoked without arguments, it prints usage information:

\begin{commandline}
scavetool <command> [options] <file>...
\end{commandline}

\subsection{The \textit{filter} Command}

The \textit{filter} command allows you to filter and/or convert result files.

A filter can be specified with the \textit{-p <filter>} option.
The filter is one or more \textit{<pattern>} or \textit{<fieldname>(<pattern>)}
expressions connected with \ttt{AND}, \ttt{OR} and \ttt{NOT} operators;
a naked \textit{<pattern>} is understood as \ttt{name(}\textit{<pattern>}\ttt{)}.
For example, the filter \ttt{"module(**.sink) AND name(delay)"} (or just
\ttt{"module(**.sink) AND delay"}) selects the \ttt{delay} vectors from all
\ttt{sink} modules.

The possible field names are:

\begin{itemize}
    \item\tbf{file}: full path of the result file
    \item\tbf{run}: run identifier
    \item\tbf{module}: module name
    \item\tbf{name}: vector name
    \item\tbf{attr:<runAttribute>}: value of an attribute of the run,
        e.g. \ttt{experiment}, \ttt{datetime} or \ttt{network}
    \item\tbf{param:<moduleParameter>}: value of the parameter in the run
\end{itemize}

Processing operations can be applied to vectors by the
\textit{-a <function>(<parameterlist>)} option. You can list
the available functions and their parameters with the \textit{info} command.

The name and format of the output file can be specified with the
\textit{-O <file>} and \textit{-F <formatname>} options, where
the format name is one of the following:

\begin{itemize}
    \item\tbf{vec}: vector file (default)
    \item\tbf{csv}: CSV file
    \item\tbf{octave}: Octave text file
    \item\tbf{matlab}: Matlab script file
\end{itemize}

The following example writes the window-averaged queuing times stored
in \ttt{in.vec} into \ttt{out.vec}:

\begin{commandline}
scavetool filter -p "queuing time" -a winavg(10) -O out.vec in.vec
\end{commandline}

The next example writes the queueing and transmission times of \ttt{sink}
modules into CSV files. It generates a separate file for each vector,
named \ttt{out-1.csv}, \ttt{out-2.csv}, etc.

\begin{commandline}
scavetool filter -p "module(**.sink) AND
                    (\"queueing time\" OR \"transmission time\")"
                 -O out.csv -F csv in.vec
\end{commandline}

The generated CSV files contain a header and two columns:

\begin{filelisting}
time,"Queue.sink.queueing time"
2.231807576851,0
7.843802235089,0
15.797137536721,3.59449
21.730758362277,6.30398
[...]
\end{filelisting}


\subsection{The \textit{index} Command}

If the index file was deleted or the vector file was modified, you need to
rebuild the index file before running the filter command:

\begin{commandline}
scavetool index Aloha-1.vec
\end{commandline}

Normally the vector data is written in blocks into the vector file.
However, if the vector file was generated by an older version of the
\cclass{cIOutputVectorManager}, it might not be so. In this case you have
to specify the -r option to rearrange the records of the vector file,
otherwise the index file would be too big and the indexing inefficient.

\subsection{The \textit{summary} Command}

The \textit{summary} command reports the list of statistics names, module names,
run ids, configuration names in the given files to the standard output.

\begin{commandline}
scavetool summary Aloha-1.vec
\end{commandline}


\section{Alternative Statistical Analysis and Plotting Tools}
\label{sec:ana-sim:alt-tools}

There are several programs and packages in addition to the {\opp} IDE
and \fprog{scavetool} that can also be used to analyze
simulation results, and create various plots and charts from them.

\begin{hint}
Our recommendation is GNU R because of its features, its popularity, and
the existence of an extension package written specifically for {\opp}
result processing.
\end{hint}


\subsection{GNU R}
\label{sec:ana-sim:gnu-r}\index{GNU R}

R is a free software environment for statistical computing and graphics.
R has an excellent programming language and powerful plotting capabilities,
and it is supported on all major operating systems and platforms.

R is widely used for statistical software development and data analysis.
The program uses a command line interface, though several graphical user
interfaces are available.

\begin{hint}
An R package for {\opp} result processing is available from
\url{https://github.com/omnetpp/omnetpp-resultfiles/wiki}.
The package supports loading the contents of {\opp} result files into R,
organizing the data and creating various plots and charts.
The package is well documented, and the web site offers a Tutorial, a Tips
page, a tutorial for the Scalar Lattice GUI package, and other information.
\end{hint}

Several other {\opp}-related packages such as SimProcTC and Syntony already
use R for data analysis and plotting.


\subsection{NumPy, SciPy and MatPlotLib}
\index{NumPy}\index{SciPy}\index{MatPlotLib}

NumPy and SciPy are numerical and scientific computing packages for the
Python programming language, and MatPlotlib is a plotting library (also for
Python).

MatPlotlib provides a ``pylab'' API designed to closely resemble that of
MATLAB, thereby making it easy to learn for experienced MATLAB users.
Matplotlib is distributed under a BSD-style license.


\subsection{MATLAB or Octave}
\index{Matlab}\index{Octave}

MATLAB is a commercial numerical computing environment and programming language.
MATLAB allows easy matrix manipulation, plotting of functions and data,
implementation of algorithms, creation of user interfaces, and interfacing
with programs in other languages.

Octave is an open-source Matlab-like package, available on nearly all platforms.
Currently Octave relies on Gnuplot for plotting, and has more limited
graphics capabilities than GNU R or MATLAB.


\subsection{Gnuplot}
\label{sec:ana-sim:gnuplot}\index{Gnuplot}

Gnuplot is a very popular command-line program that can generate two-
and three-dimensional plots of functions and data. The program runs
on all major platforms, and it is well supported.

Gnuplot has an interactive command interface. For example, if you have
the data files \texttt{foo.csv} and \texttt{bar.csv} that contain
two values per line ($x$ $y$; such files can be exported with
\fprog{scavetool} from vector files), you can plot them in the same
graph by typing:

\begin{commandline}
plot "foo.csv" with lines, "bar.csv" with lines
\end{commandline}

To adjust the $y$ range, you would type:

\begin{commandline}
set yrange [0:1.2]
replot
\end{commandline}

Several commands are available to adjust ranges, plotting style, labels,
scaling etc. On Windows, you can copy the resulting graph to the clipboard from
the Gnuplot window's system menu, then insert it into the application you
are working with.


\subsection{ROOT}
\index{ROOT}

\textit{ROOT} is an object-oriented data analysis framework,
with strong support for plotting and graphics in general.
ROOT was developed at CERN, and is distributed under a BSD-like license.

ROOT is based on \textit{CINT}, a ``C/C++ interpreter''
aimed at processing C/C++ scripts. It is probably harder to get started
using ROOT than with either Gnuplot or Grace, but you will find that ROOT provides
power and flexibility that would be unattainable with the other two programs.

Curt Brune's page at Stanford (http://www.slac.stanford.edu/\~curt/omnet++/)
shows examples of what you can achieve using ROOT with {\opp}.


\subsection{Grace}
\index{Grace}

An ``honorable mention,'' \textit{Grace} is a powerful GPL data visualization
program with a menu-and-dialog graphical user interface for X and Motif. It
has also been ported to Windows. Grace is also known as \textit{xmgrace},
and it is a successor of \textit{ACE/gr} or \textit{Xmgr}.

Grace can export graphics in various raster and vector formats, and has
many useful features like built-in statistics and analysis functions (e.g.
correlation, histogram), fitting, splines, etc., and it also has a built-in
programming language.


\subsection{Spreadsheet Programs}
\index{Spreadsheets}

One straightforward solution is to use spreadsheets such as OpenOffice
Calc, Microsoft Excel, Gnumeric or Calligra Tables (formerly KSpread).
Data can be imported from CSV or other formats, exported with \fprog{scavetool}
(see \ref{sec:ana-sim:scavetool}).

Spreadsheets have good charting and statistical features. A useful
functionality spreadsheets offer for analyzing scalar files is
\textit{PivotTable} (Excel) or \textit{DataPilot} (OpenOffice). The
drawback of using spreadsheets is limited automation, leading to tedious
and repetitive tasks; also, the number of rows is usually limited to about
32,000..64,000, which can be limiting when working with large vector files.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "usman"
%%% End:



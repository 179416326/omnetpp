<?xml version="1.0"?>
<!DOCTYPE chapter SYSTEM "custom-docbook.dtd">
<chapter id="result-anal" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Analyzing the Results</title>
  <sect1>
    <title>Overview</title>
    <para> Analyzing the simulation result is a lengthy and time consuming process. The result of the
      simulation is recorded as scalar values, vector values and histograms. The user then applies statistical
      methods to extract the relevant information and to draw a conclusion. This process may include several
      steps. Usually you need to filter and transform the data, and chart the result. Automation is
      very important here. You do not want to repeat the steps of recreating charts every time
      you rerun simulations.</para>
    <para>
      In &Omnetpp;
      4.x, the statistical analysis tool is integrated into the Eclipse environment. Your settings (i.e. your
      recipe for finding results from the raw data) will be recorded in analysis files (.anf) and will become
      instantly reproducible. This means that all processing and charts are stored as datasets; for example,
      if simulations need to be rerun due to a model bug or misconfiguration, existing charts need not be
      recreated all over again. Simply replacing the old result files with the new ones will result in
      the charts being automatically displayed with the new data.
    </para>
    <para> When creating an analysis, the user first selects the input of the analysis by specifying file names
      or file name patterns (e.g. "adhoc-*.vec"). Data of interest can be selected into datasets using additional
      pattern rules. The user can define datasets by adding various processing, filtering and charting steps;
      all using the GUI. Data in result files are tagged with meta information. Experiment, measurement and
      replication labels are added to the result files to make the filtering process easy. It is possible to
      create very sophisticated filtering rules (e.g. all 802.11 retry counts of host[5..10] in
      experiment X, averaged over replications). In addition, datasets can use other datasets as their input so
      datasets can build on each other.</para>
  </sect1>
  <sect1>
    <title>Creating Analysis Files</title>
    <para> To create a new analysis file, choose `` File | New | Analysis File '' from the menu. Select the
      folder for the new file and enter the file name. Press ``Finish'' to create and open an empty
      analysis file.</para>
    <picture file="pictures/ANF-NewAnalysisFileDialog.png">New Analysis File dialog</picture>
    <para> There is a quick way to create an analysis file for a result file. Just double-click on the result
      file in the ``Project Explorer View'' to open the ``New Analysis File'' dialog. The folder and file name
      is prefilled according to the location and name of the result file. Press ``Finish'' to create a new analysis
      file containing the vector and scalar files whose names correspond to the result file. If
      the name of the result file contains a numeric suffix (e.g. [[aloha-10.vec]]), then all files with the same
      prefix will be added to the analysis file (i.e. [[aloha-*.vec]] and [[aloha-*.sca]]).</para>
    <tip>
      <para> If the analysis file already exists, double-clicking on the result file will open it.</para>
    </tip>
  </sect1>
  <sect1>
    <title>Using the Analysis Editor</title>
    <para> The Analysis Editor is implemented as a multi-page editor. What the editor edits is the "recipe":
      what result files to take as inputs, what data to select from them, what (optional) processing steps to
      apply, and what kind of charts to create from them. The pages (tabs) of the editor roughly correspond to
      these steps.</para>
    <sect2>
      <title>Input Files</title>
      <sect3>
        <title>Selecting input files</title>
        <para> The first page displays the result files that serve as input for the analysis. The upper half
          specifies what files to select using explicit filenames or wildcards. New input files can be added
          to the analysis by dragging vector and scalar files from the ``Project Explorer View'', or by
          opening dialogs with the ``Add File...'' or ``Wildcard...'' buttons. If the file name starts with
          '/,' it is interpreted relative to the workspace root; otherwise, it is relative to the folder of the
          analysis file.</para>
        <picture file="pictures/ANF-InputsPage.png"> Specifying input files for data analysis</picture>
        <para> The input files are loaded when the analysis file is opened. When the file changes on the disk,
          it is reloaded automatically when the workspace is refreshed (Eclipse refreshes the workspace
          automatically if the ``General|Workspace|Refresh automatically'' option is turned on in the
          Preferences). Vector files are not loaded directly; instead, an index file is created and the vector
          attributes (name, module, run, statistics, etc.) are loaded from the index file. The index files are
          generated during the simulation, but can be safely deleted without loss of information. If the index
          file is missing or the vector file was modified, the IDE rebuilds the index in the background.
        </para>
        <tip>
          <para> The ``Progress View'' displays the progress of the indexing process.</para>
        </tip>
        <para>
          The lower half shows what files matched the input specification and what runs they
          contain. Note that &Omnetpp;
          4.x result files contain a unique run ID and several metadata annotations in addition to the actual
          recorded data. The third tree organizes simulation runs according to their
          experiment-measurement-replication labels.
        </para>
        <para>
          The underlying assumption is that users will organize their simulation-based research into various
          ``experiments''
          . An experiment will consist of several
          ``measurements'',
          which are typically (but not necessarily) simulations done with the same model but with different
          parameter settings (i.e. the user will explore the parameter space with several simulation runs).
          To gain statistical confidence in the results, each measurement may be repeated
          several times with different random number seeds. It is easy to set up such scenarios with the
          improved INI files of &Omnetpp;
          4.x. Then, the experiment-measurement-replication labels will be assigned automatically (Note: please
          refer to the chapter "Configuring and Running Simulations" in the manual for more discussion).
        </para>
      </sect3>
      <sect3>
        <title>Browsing input</title>
        <para> The second page of the Analysis editor displays results (vectors, scalars and histograms) from
          all files in tables and lets the user browse them. Results can be sorted and filtered. Simple
          filtering is possible with combo boxes, or when that is not enough, the user can write arbitrarily
          complex filters using a generic pattern-matching expression language. Selected or filtered data can
          be immediately plotted, or remembered in named datasets for further processing.</para>
          <tip>You can switch between the ``All'', ``Vectors'', ``Scalars'' and ``Histograms'' pages using the 
            underlined keys (<keycap>Alt+KEY</keycap> combination) or the <keycap>Ctrl+PgUp</keycap> and 
            <keycap>Ctrl+PgDown</keycap> keys.</tip>
        <para> Pressing the ``Advanced'' button switches to advanced filter mode. In the text field, you can
          enter a complex filter pattern.</para>
        <tip>
          <para> You can easily display the data of a selected file, run, experiment, measurement or
            replication if you double-click on the required tree node in the lower part of the ``Inputs'' page. It
            sets the appropriate filter and shows the data on the ``Browse Data'' page.</para>
          <para> If you right-click on a table cell and select the ``Set filter: ...'' action from the menu, you
            can set the value of that cell as the filter expression.</para>
        </tip>
        <picture file="pictures/ANF-BrowseDataPage.png"> Browsing vector and scalar data generated by the simulation</picture>
        <para> To hide or show table columns, open ``Choose table columns...'' from the context menu and
          select the columns to be displayed. The settings are persistent and applied in each subsequently
          opened editor. The table rows can be sorted by clicking on the column name.</para>
        <para> You can display the selected data items on a chart. To open the chart, choose ``Plot'' from the
          context menu (double-click also works for single data lines). The opened chart is not added
          automatically to the analysis file, so you can explore the data by opening the chart this way and
          closing the chart page without making the editor "dirty."</para>
        <para> The selected vector's data can also be displayed in a table. Make sure that the ``Output Vector
          View'' is opened. If it is not open, you can open it from the context menu (``Show Output Vector View'').
          If you select a vector in the editor, the view will display its content.</para>
        <para> You can create a dataset from the selected result items. Select ``Add Filter Expression to
          Dataset...'' if you want to add all items displayed in the table. Select ``Add Filter Selected Data
          to Dataset...'' if you want to add the selected items only. You can add the items to an existing
          dataset, or you can create a new dataset in the opening dialog.</para>
          <tip>You can switch between the ``Inputs'', ``Browse Data'' and ``Dataset'' pages using the 
            <keycap>Alt+PgUp</keycap> and <keycap>Alt+PgDown</keycap> keys.</tip>
      </sect3>
      <sect3>
        <title>Filter expressions</title>
        <para>
          A filter expression is composed of atomic patterns with the AND, OR, NOT operators. An atomic
          pattern filters for the value of a field of the result item and has the form <![CDATA[<field_name>(<pattern>)]]>.
          The following table shows the valid field names. You can omit the name field and simply use the name
          pattern as a filter expression. It must be quoted if it contains whitespace or
          parentheses.
          <informaltable>
            <tgroup cols="2">
              <colspec colwidth="1in" />
              <colspec colwidth="2*" /> <!-- There is a warning if 1* is used, bug in docbook/fo? -->
              <thead>
                <row>
                  <entry>Field</entry>
                  <entry>Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>name</entry>
                  <entry>the name of the scalar, vector or histogram</entry>
                </row>
                <row>
                  <entry>module</entry>
                  <entry>the name of the module</entry>
                </row>
                <row>
                  <entry>file</entry>
                  <entry>the file of the result item</entry>
                </row>
                <row>
                  <entry>run</entry>
                  <entry>the run identifier</entry>
                </row>
                <row>
                  <entry>attr: ``name''</entry>
                  <entry>the value of the run attribute named ``name'', e.g. attr:experiment</entry>
                </row>
                <row>
                  <entry>param: ``name''</entry>
                  <entry>the value of the module parameter named ``name''</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          In the pattern specifying the field value, you can use the following shortcuts:
          <informaltable>
            <tgroup cols="2">
              <colspec colwidth="1in" />
              <colspec colwidth="2*" />
              <thead>
                <row>
                  <entry>Pattern</entry>
                  <entry>Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>?</entry>
                  <entry>matches any character except '.'</entry>
                </row>
                <row>
                  <entry>*</entry>
                  <entry>matches zero or more characters except '.'</entry>
                </row>
                <row>
                  <entry>**</entry>
                  <entry>matches zero or more characters (any character)</entry>
                </row>
                <row>
                  <entry>{a-z}</entry>
                  <entry>matches a character in range a-z</entry>
                </row>
                <row>
                  <entry>{^a-z}</entry>
                  <entry>matches a character not in range a-z</entry>
                </row>
                <row>
                  <entry>{32..255}</entry>
                  <entry>any number (i.e. sequence of digits) in range 32..255 (e.g. "99")</entry>
                </row>
                <row>
                  <entry>[32..255]</entry>
                  <entry>any number in square brackets in range 32..255 (e.g. "[99]")</entry>
                </row>
                <row>
                  <entry>\</entry>
                  <entry>takes away the special meaning of the subsequent character</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>
        <tip>
          <para>
            Content Assist is available in the text fields where you can enter a filter expression. Press
            <keycombo><keycap>Ctrl</keycap><keycap>Space</keycap></keycombo>
            to get a list of appropriate suggestions related to the expression at the cursor position.
          </para>
        </tip>
      </sect3>
      <sect3>
        <title>Examples</title>
        <informalexample>
          <para>
            <literallayout><userinput>"queuing time"</userinput></literallayout>
          </para>
          <para>
            filters for result items named ``queuing time''.
          </para>
        </informalexample>
        <informalexample>
          <para>
            <literallayout><userinput>module(**.sink) AND (name("queuing time") OR</userinput>
<userinput>                     name("transmission time"))</userinput></literallayout>
          </para>
          <para>
            results in the ``queuing times'' and ``transmission times'' that are written by modules named ``sink''.
          </para>
        </informalexample>
      </sect3>
    </sect2>
    <sect2>
      <title>Datasets</title>
      <sect3>
        <title>Overview</title>
        <para> The third page displays the datasets and charts created during the analysis. Datasets describe
          a set of input data, the processing applied to them and the charts. The dataset is displayed as a
          tree of processing steps and charts. There are nodes for adding and discarding data, applying
          processing to vectors and scalars, selecting the operands of the operations and content of charts,
          and for creating charts.</para>
        <picture file="pictures/ANF-DatasetsPage.png">Defining datasets to be analyzed</picture>
        <tip>
          <para> You can browse the dataset's content in the ``Dataset View''. Open the view by selecting
            ``Show Dataset View'' from the context menu. Select a chart to display its content or another node
            to display the content of the dataset after processing is applied.</para>
        </tip>
      </sect3>
      <sect3>
        <title>Editing datasets</title>
        <para> The usual tree editing operations work on the Dataset tree. New elements can be added by
          dragging elements from the palette on the right to an appropriate place on the tree. Alternatively,
          you can select the parent node and press the button on the toolbar. An existing element can be
          edited by selecting the element and editing its properties on the property sheet, or by opening an
          item-specific edit dialog by choosing ``Properties...'' from the context menu.</para>
        <tip>
          <para> Datasets can be opened on a separate page by double-clicking on them. It is easier
            to edit the tree on this page. Double-clicking a chart node will display the chart.</para>
        </tip>
        <para>
          Computations can be applied to the data by adding Apply to Vectors/Compute Vectors/Compute Scalars
          nodes to the dataset. The input of the computations can be selected by adding Select/Deselect children to the
          processing node. By default, the computation input is the whole content of the dataset at the processing node.
          Details of the computations are described in the next sections.
        </para>
        <para> Processing steps within a Group node only affect the group. This way, you can
          create branches in the dataset. To group a range of sibling nodes, select them and choose ``Group''
          from the context menu. To remove the grouping, select the Group node and choose ``Ungroup''.</para>
        <para>
          Charts can be inserted to display data. The data items to be displayed can be selected by adding
          Select/Deselect children to the chart node. By default, the chart displays all data in the dataset at
          its position. You can modify the content of the chart by adding Select and Deselect children to it.
          Charts can be fully customized including setting titles, colors, adding legends, grid lines, etc.
          See the
          <xref linkend="charts" />
          section for details.
        </para>
      </sect3>
      <sect3>
        <title>Computing Vectors</title>
        <para>
          Both Compute Vectors and Apply to Vectors nodes compute new vectors from other vectors.
          The difference between them is that Apply to Vectors will remove
          its input from the dataset, while Compute keeps the original data, too.
        </para>
        <para>
          <xref linkend="processing-operations" /> contains the list of available operations on vectors.
          <xi:include href="processing-operations.xml#xpointer(//table[@id='processing-operations'])" />
        </para>
      </sect3>
      <sect3>
        <title>Computing Scalars</title>

        <para>The Compute Scalars dataset node adds new scalars to the dataset whose values are computed
        from other statistics in the dataset. The input of the computation can be restricted by adding
        Select/Deselect nodes under it.</para>

        <picture file="pictures/ANF-EditComputeScalars.png" width="80%">Edit 'Compute Scalars' dialog</picture>

        <para>In the ``Properties'' dialog of the Compute Scalars node, you can set the name and module of the generated
        scalars, and the expression that computes their values. You can also can enter a grouping expression,
        and set flags to average the values across replications.
        Content Assist (<keycap>Ctrl+SPACE</keycap>) is available for the ``Value'' field, it can
        propose statistic and function names.</para>

        <para>The content of the dialog is validated after each keystroke, and errors are displayed as small
        icons next to the edit field. Hovering over the error icon shows the error message. However, not all errors
        can be detected statically, in the dialog. If an error occurs while performing the computation, then an error
        marker is added to the analysis file and to the corresponding dataset node in the Analysis editor.
        You can view the error in the ``Problems View'', and double-clicking the problem item navigates back to the
        ``Compute Scalars'' node.</para>

        <para>When a ``Compute Scalars'' node in the dataset is evaluated,
        computations will use the fields rougly in the order they appear in the dialog.
        First, grouping takes place (by simulation run, and then by the optional grouping expression);
        then values are computed; then values are stored by the given name and module; and finally,
        averaging across simulation runs takes place.</para>

        <para>Explanation the dialog fields:</para>

        <formalpara><title>Value.</title>
        This is an arithmetic expression for the value of the generated scalar(s). You can use
        the values of existing scalars, various properties of existing vectors and histograms
        (mean, min, max, etc), normal and aggregate functions (mean, min, max, etc),
        and the usual set of operators.</formalpara>

        <para>To refer to the value of a scalar, simply use the scalar name (e.g. [[pkLoss]]),
        or enclose it with apostrophes if the name of the scalar contains special characters
        (e.g. [['rcvdPk:count']].) If there are several scalars with that name in the
        input dataset, usually recorded by different modules or in different runs, then the computation
        will be performed on each. The scalar name cannot contain wildcards or dollar variables (see later.)</para>

        <para>When necessary, you can qualify the scalar name with a module name pattern that will be
        matched against the full paths of modules. The same pattern syntax is used as in ini files and
        in other parts of the Analysis Tool (Quick reminder: use [[*]] and [[**]] for wildcards, [[{5..10}]]
        for numeric range, [[[5..10]]] for module index range). If multiple scalars match
        the qualified name, the expression will be computed for each. If there are several such patterns
        in the expression, then the computation will be performed on their Cartesian product.</para>

        <para>If the expression mentions several unqualified scalars (i.e. without module pattern),
        they are expected to come from the same module. For example, if your expression is [[foo+bar]]
        but the [[foo]] and [[bar]] scalars have been recorded by different modules, the result will be empty.</para>

        <para>The iteration can be restricted by binding some part of the module name to variables,
        and use those variables in other patterns. The [[${x=&lt;pattern&gt;}]] syntax in a module
        name pattern binds the part of the module name matched to ``pattern'' to a variable named [[x]].
        These variables can be referred as [[${x}]] in other patterns. The [[${...}]] syntax also allows
        you to embed expressions (e.g. [[${x+1}]]) into the pattern.</para>

        <para>To make use of vectors and histograms in the computation, you can use the [[count()]],
        [[mean()]], [[min()]], [[max()]], [[stddev()]] and [[variance()]] functions to access their properties
        (e.g. [[count(**.mac.pkDrop)]] or [[max('end-to-end delay')]]).</para>

        <para>The following functions can be applied to scalars or to an expression that yields a scalar value:
        [[count()]], [[mean()]], [[min()]], [[max()]], [[stddev()]], [[variance()]].
        These aggregate functions produce one value for a group of scalars instead of one for each scalar.
        By default, each scalar belongs to the same group, but it is possible to group them by module name (see Grouping).
        Aggregate functions cannot cross simulation run boundaries, e.g they cannot be used to compute average over
        all replications of the same configuration; use the ``Average replications'' checkbox for that.</para>

        <formalpara><title>Grouping.</title>
        Scalars can be grouped by module or value before the value computation, and you can enter a grouping
        expression here. Grouping is the most useful when you want to use aggregate functions 
        ([[count()]], [[mean()]], etc.) in the value expression.</formalpara>

        <para>The grouping expression is evaluated for each statistic in the input dataset, and the resulting value
        denotes the statistic's group. For example, if the expression produces 0 for some statistics
        and 1 for others, there will be two groups. Aggregate functions ([[count()]], [[mean()]], etc.)
        are performed on each group independently.</para>

        <para>In the grouping expression, you can refer to the name, module and values of the statistic
        ([[module]], [[name]] and [[value]]; the latter is only meaningful on scalars, and produces ``NaN''
        for histograms and vectors), and to attributes of the simulation run ([[run]], [[configname]],
        [[runnumber]], [[experiment]], [[measurement]], [[replication]], iteration variables of the ini file, etc.)
        However, note that run attributes are not as useful as they would appear, because grouping
        only takes place within simulation runs, you cannot join data from several simulation runs
        into one group this way.</para>

        <para>Often you want derive the group identifier from some part of the module name.
        A useful tool for that is the pattern matching operator ([[=~]]) combined with conditionals ([[? :]]).
        The expression [[&lt;str&gt; =~ &lt;pat&gt;]] matches the string [[str]] with the pattern [[pat]].
        If there is no match, the value of the expression is [[false]], otherwise the input string [[str]]
        (which counts as true). The useful bit is that you can bind parts of the matching string to variables
        with the [[${x=&lt;pattern&gt;}]] syntax in the pattern (see above), and you can use those variables
        later in the grouping expression, and also in the ``Value'', ``Name'' and ``Module'' fields.</para>

        <formalpara><title>Name.</title>
        This is the name for the generated scalars. You can enter a literal string here.
        You can also use dollar variables bound in the ``Value'' and ``Grouping'' fields (e.g. [[${x}]]),
        and their expressions (e.g. [[${x+y+1}]]).</formalpara>

        <formalpara><title>Module.</title>
        This is the place where you can enter the module name for the newly computed scalars.
        If the value expression contains unqualified scalars (those without module name patterns)
        that are not subject to aggregate functions, and you agree to place
        the new scalars into the same modules as theirs, then this field can be left empty.
        Otherwise, enter the module name. You can use dollar variables bound in the ``Value'' and
        ``Grouping'' fields (e.g. [[${x}]]), and their expressions (e.g. [[${x+y+1}]]).
        Note that the module name does not need to be an existing module name; you can "make up"
        new modules by entering arbitrary names here.</formalpara>

        <formalpara><title>Average replications checkbox.</title>
        Check to compute only the avarage value across repetitions.</formalpara>

        <para>The computation is performed in each run independently by default. If some run is a replication of the same
        measurement with different seeds, you may want to average the results. If the ``Average replications'' checkbox is
        selected, then only one scalar is added to the dataset for each measurement.</para>

        <para>A new run generated for the scalar which represents the set of replications which it is computed from.
        The attributes of this run are those that have the same value in all replications.</para>

        <formalpara><title>Other checkboxes.</title>
        In addition to mean, you can also add other statistical properties of the computed scalar to the dataset
        by selecting the corresponding checkboxes in the dialog. The names of these new scalars
        will be formed by adding the [[:stddev]], [[:confint]], [[:min]], or [[:max]] suffix
        to the base name of the scalar.</formalpara>

        <note>A more formal description of the ``Compute Scalars'' feature's operation, together with the
        list of available functions and other details, can be found in the
        Appendix (see <xref linkend="computed-scalar-spec"/>).</note>

        <formalpara><title>Examples.</title>
        Let us illustrate the usage of the computations with some examples.</formalpara>

        <orderedlist>

          <listitem><para>Assume that you have several source modules in the network that generate CBR traffic,
          parameterized with packet length (in bytes) and send interval (seconds). Both parameters
          are saved as scalars by each module ([[pkLen]], [[sendInterval]]), but you want to use the bit rate
          for further computations or charts. Adding a ``Compute Scalar'' node with the following content
          will create an additional [[bitrate]] scalar for each source module:</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[pkLen*8/sendInterval]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[bitrate]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>Assume that several sink modules record [[rcvdByteCount]] scalars, and the simulation
          duration is saved globally as the [[duration]] scalar of the toplevel module. We are interested
          in the throughput at each sink module. In the value expression we can refer to the [[duration]] scalar
          by its qualified name, i.e. prefix it with the full name of its module. [[rcvdByteCount]] can be
          left unqualified, and then the ``Module'' field doesn't need to be filled out because it will
          be inferred from the [[rcvdByteCount]] statistic.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[8*rcvdByteCount / Network.duration]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[throughput]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>If you are interested in the total number of bytes received in the network,
          you can use the [[sum()]] function. In this example we store the result as a new scalar
          of the toplevel module, [[Network]].</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[sum(rcvdByteCount)]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[totalRcvdBytes]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Network]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>If several modules record scalars named [[rcvdByteCount]] but you are only
          interested in the ones recorded from network hosts, you can qualify the scalar name with
          a pattern:</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[sum(**.host*.**.rcvdByteCount)]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[totalHostRcvdBytes]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Network]]</para></listitem>
          </itemizedlist></para>

          <note>An alternative solution would be to restrict the input of the Compute Scalars node
          by adding a Select child under it.</note>

          </listitem>

          <listitem><para>If several modules record vectors named [[end-to-end delay]] and you are
          interested in the average of the peak end-to-end delays experienced by each module,
          you can use the [[max()]] function on the vectors to get the peak, then [[mean()]]
          to obtain their averages. Note that the vector name needs to be quoted with apostrophes
          because it contains spaces.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[mean(max('end-to-end delay'))]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[avgPeakDelay]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Network]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>Let's assume there are 3 clients ([[cli0, cli1, cli2]]) and 3 servers ([[srv0, srv1, srv2]])
          in the network, and each client sends datagrams to the corresponding server. The packet loss
          per client-server pair can be computed from the number of sent and received packets.
          We use the [[i]] variable to match the corresponding clients and servers.
          (Without the [[i]] variable, i.e. by writing just [[Net.cli*.pkSent - Net.srv*.pkRcvd]],
          the result would be Cartesian product.)</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[Net.cli${i={0..2}}.pkSent - Net.srv${i}.pkRcvd]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[pkLoss]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Net.srv${i}]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>A similar example is when you want to compute the total number of transport
          packets (the sum of the TCP and UDP packet counts) for each host. Since the input scalars
          are recorded by different modules, we need the [[host]] variable to match TCP and UDP modules
          under the same host.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[${host=**}.udp.pkCount + ${host}.tcp.pkCount]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[transportPkCount]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[${host}]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>This example is a slight modification of the previous example. Assume that the TCP
          module writes an output vector (named [[pkSent]]) containing the length of each packet sent,
          and the UDP module writes a histogram of the sent packet lengths. As in the previous example,
          we want to compute the number of sent packets for each host; we use [[count()]] to
          extract the number of values from histograms and output vectors.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[count(${host=**}.udp.pkSent) + count(${host}.tcp.pkSent)]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[pkCount]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[${host}]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>Now we are computing the average number of data packets sent by the hosts. We will use
          the [[mean()]] function in the ``Value'' expression: [[mean(${host=**}.udp.pkCount + ${host}.tcp.pkCount)]].
          The [[mean]] function computes one value from a set of values. Because this value can not be associated
          with one host, the [[host]] variable will be undefined outside the [[mean()]] function call. Therefore
          you can not enter [[${host}]] into the ``Module'', but an appropriate module name should be choosen.
          Important: the [[mean()]] function cannot be used to compute the average of values that come from different
          runs, as the ``Value'' expression is always evaluated with input statistics that come from the
          same run; you have to use the ``Average replications''  checkbox for that instead.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[mean(${host=**}.udp.pkCount + ${host}.tcp.pkCount))]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[avgPkCount]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Network]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>Again, we are interested in the average number of sent packets, but we want
          to compute the average for each subnet. In SQL you would use the GROUP BY clause to generate that report,
          here you can use the ``Grouping'' expression. Let us assume that the subnets are at the second level
          of the module hierarhcy, so they can be identified by the second component of the full names of modules.
          Giving [[(module =~ *.${subnet=*}.**) ? subnet : "n/a"]] as the ``Grouping'' expression, the group identifier
          will be the subnet of the modules (and [[n/a]] for the network). Now the same ``Value'' expression as
          in the previous example computes the average for each subnet. The [[group]] variable now contains the
          name of the subnet, so you can use the [[${group}]] expression as the ``Module''.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[mean(${host=**}.udp.pkCount + ${host}.tcp.pkCount))]]</para></listitem>
          <listitem><para><emphasis role="bold">Grouping</emphasis>: [[(module =~ *.${subnet=*}.**) ? subnet : "n/a"]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[avgPkCount]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[${group}]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>It is also possible to group the scalars by their values.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[count(responseTime)]]</para></listitem>
          <listitem><para><emphasis role="bold">Grouping</emphasis>: [[value > 1.0 ? "Large" : "Normal"]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[num${group}ResponseTimes]]</para></listitem>
          <listitem><para>``or:'' [[${"num" ++ group ++ "ResponseTimes"}]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Network]]</para></listitem>
          </itemizedlist></para>

          <note>Note the use of [[++]] for string concatenation in the name expression. The normal [[+]] operator
          does numeric addition, and it would attempt to convert both operands to a numbers beforehand.</note>

          </listitem>


          <listitem><para>Assume that you run the simulation with the same parameter settings, but different seeds,
          i.e. you have runs that are replications of the same measurement. Then computations are repeated for each
          run, therefore they produce values for each replications. If you are not interested in the individual
          results in the replications, you can generate only their average by turning on the ``Average replications''
          checkbox. The average will be saved with the name entered into the ``Name'' field. The minimum/maximum
          value, the standard deviation of the distribution, and the confidence interval of the mean can also
          be generated. Their name will contain a [[:min]], [[:max]], [[:stddev]], [[:confint]] suffix.</para></listitem>


          <listitem><para>To add a constant value as a scalar, enter the value into ``Value'' field,
          its name into the ``Name'', and the module name into the ``Module'' field. Note that you can
          use any name for the target module, not just names of existing modules. As a result of the computation,
          one scalar will be added in every run that is present in the input dataset.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[299792458]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[speedOfLight]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Network.channelControl]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>If you want to add the constant for each module in your dataset, then enter [[module]] into
          the ``Grouping'' field, and [[${group}]] into the ``Module'' field. In this case
          the statistics of the input dataset are grouped not only by their run, but by their module name too.
          The expression entered into the ``Grouping'' field is a predefined variable, that refers
          to the module name of the statistic which the group identifier is generated for. We refer to the
          value of the group identifier (i.e. the module name) in the ``Module'' field.
          </para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[3.1415927]]</para></listitem>
          <listitem><para><emphasis role="bold">Grouping</emphasis>: [[module]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[pi]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[${group}]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>Sometimes multiple steps are needed to compute what you need. Assume you have
          a network where various modules record ping round-trip delays (RTT), and you want to count the modules
          with large RTT values (where the average RTT is more than twice the global average in the network).
          The following examples achieves that. Step 1 and 2 could be merged, but we left them separate 
          for better readability.</para>

          <para>Step 1:</para>
          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[mean('rtt:vector')]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[average]]</para></listitem>
          </itemizedlist></para>

          <para>Step 2:</para>
          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[average / mean(**.average)]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[relativeAverage]]</para></listitem>
          </itemizedlist></para>

          <para>Step 3:</para>
          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[count(relativeAverage)]]</para></listitem>
          <listitem><para><emphasis role="bold">Grouping</emphasis>: [[value > 2.0  ? "Above" : "Normal"]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[num${group}]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Net]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>In this example, we have 100 routers ([[Net.rte[0]..Net.rte[99] ]]) that
          all record the number of dropped packets ([[drops]] scalar), and we want to know how many routers
          dropped 0..99 packets, 100..199 packets, 200..299 packets, and so on. For this we group the scalars
          by their values, and count the scalars in each group.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[count(drops)]]</para></listitem>
          <listitem><para><emphasis role="bold">Grouping</emphasis>: [[floor(value / 100)]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[values in the ${group}00- group]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Net]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>Assume we have the same 100 routers with drop count scalars as in the previous example.
          We want to group the routers in batches of 10 by index, and compute the average drop count in each batch.</para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[mean(drops)]]</para></listitem>
          <listitem><para><emphasis role="bold">Grouping</emphasis>: [[(module=~ *.rte[${index=*}]) ? floor(index/10) : "n/a"]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[avgDropsInGroup${group}]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Net]]</para></listitem>
          </itemizedlist></para>

          </listitem>

          <listitem><para>This is a variation of the previous example: if the batches are not of equal size, we can 
          use the [[locate(x,a1,a2,a3,...an)]] function to group them. [[locate()]] returns the index of the first element
          of ``a1..an'' that is less or equal than ``x'' (or 0 if ``x &lt; a1''). </para>

          <para><itemizedlist spacing="compact" mark="none">
          <listitem><para><emphasis role="bold">Value</emphasis>: [[mean(drops)]]</para></listitem>
          <listitem><para><emphasis role="bold">Grouping</emphasis>:</para></listitem> <!-- line too long, adding a "manual line break" -->
          <listitem><para>  [[(module=~ *.rte[${index=*}]) ? locate(index,5,15,35,85) : "n/a"]]</para></listitem>
          <listitem><para><emphasis role="bold">Name</emphasis>: [[avgDropsInGroup${group}]]</para></listitem>
          <listitem><para><emphasis role="bold">Module</emphasis>: [[Net]]</para></listitem>
          </itemizedlist></para>

          </listitem>

        </orderedlist>
      </sect3>

      <sect3>
        <title>Export</title>
        <para> You can export the content of the dataset into text files. Three formats are supported: comma
          separated values (CSV), Octave text files and Matlab script. Right-click on the processing node or
          chart, and select the format from the ``Export to File'' submenu. The file will contain the data of
          the chart, or the dataset after the selected processing is applied. Enter the name of the file and
          press ``Finish''.</para>
        <para>
          Vectors are always written as two columns into the CSV files, but the shape of the scalars table
          can be changed by selecting the grouping attributes in the dialog. For example, assume we have two scalars
          (named s1 and s2) written by two modules (m1 and m2) in two runs (r1 and r2), resulting in a total of 8
          scalar values. If none of the checkboxes is selected in the ``Scalars grouped by'' group, then the
          data is written as:
          <informaltable>
            <tgroup cols="8">
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <thead>
                <row>
                  <entry>r1 m1 s1</entry>
                  <entry>r1 m1 s2</entry>
                  <entry>r1 m2 s1</entry>
                  <entry>r1 m2 s2</entry>
                  <entry>r2 m1 s1</entry>
                  <entry>r2 m1 s2</entry>
                  <entry>r2 m2 s1</entry>
                  <entry>r2 m2 s2</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>1</entry>
                  <entry>2</entry>
                  <entry>3</entry>
                  <entry>4</entry>
                  <entry>5</entry>
                  <entry>6</entry>
                  <entry>7</entry>
                  <entry>8</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          Grouping the scalars by module name and scalar name would have the following result:
          <informaltable pgwide="0">
            <tgroup cols="4">
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <colspec colwidth="48pt" />
              <thead>
                <row>
                  <entry>Module</entry>
                  <entry>Name</entry>
                  <entry>r1</entry>
                  <entry>r2</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>m1</entry>
                  <entry>s1</entry>
                  <entry>1</entry>
                  <entry>5</entry>
                </row>
                <row>
                  <entry>m1</entry>
                  <entry>s2</entry>
                  <entry>2</entry>
                  <entry>6</entry>
                </row>
                <row>
                  <entry>m2</entry>
                  <entry>s1</entry>
                  <entry>3</entry>
                  <entry>7</entry>
                </row>
                <row>
                  <entry>m2</entry>
                  <entry>s2</entry>
                  <entry>4</entry>
                  <entry>8</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>
        <para>The settings specific to the file format are:</para>
        <formalpara>
          <title>CSV</title>
          <para> You can select the separator, line ends and quoting character. The default setting
            corresponds to RFC4180. The precision of the numeric values can also be set. The CSV file contains
            an optional header followed by the vector's data or groups of scalars. If multiple vectors are
            exported, each vector is written into a separate file.</para>
        </formalpara>
        <formalpara>
          <title>Octave</title>
          <para>
            The data is exported as an Octave text file. This format can be loaded into the
            <ulink url="http:://www.r-project.org">R</ulink>
            statistical data analysis tool, as well. The tables are saved as structures containing an array for
            each column.
          </para>
        </formalpara>
        <formalpara>
          <title>Matlab</title>
          <para> The data is exported as a Matlab script file. It can be loaded into Matlab/Octave with the
            source() function.</para>
        </formalpara>
      </sect3>
      <sect3>
        <title>Chart sheets</title>
        <para> Sometimes, it is useful to display several charts on one page. When you create a chart, it is
          automatically added to a default chart sheet. Chart sheets and the their charts are displayed on the
          lower pane of the ``Datasets'' page. To create a new chart sheet, use the ``Chart Sheet'' button on the
          palette. You can add charts to it either by using the opening dialog or by dragging charts. To move
          charts between chart sheets, use drag and drop or Cut/Paste. You can display the charts by
          double-clicking on the chart sheet node.</para>
        <picture file="pictures/ANF-ChartSheetPage.png">Chart Sheet page with three charts</picture>
      </sect3>
    </sect2>
    <sect2 id="charts" xreflabel="Charts">
      <title>Charts</title>
      <sect3>
        <title>Overview</title>
        <para>
          You typically want to display the recorded data in charts. In &Omnetpp;
          4.x, you can open charts for scalar, vector or histogram data with one click. Charts can be saved
          into the analysis file, too. The Analysis Editor supports bar charts, line charts, histogram charts
          and scatter charts. Charts are interactive; users can zoom, scroll, and access tooltips that give
          information about the data items.
        </para>
        <para> Charts can be customized. Some of the customizable options include titles, fonts, legends, grid
          lines, colors, line styles, and symbols.</para>
      </sect3>
      <sect3>
        <title>Creating charts</title>
        <para> To create a chart, use the palette on the ``Dataset'' page. Drag the chart button and drop it to
          the dataset at the position you want it to appear. If you press the chart button then it opens a
          dialog where you can edit the properties of the new chart. In this case the new chart will be added
          at the end of the selected dataset or after the selected dataset item.</para>
        <para>
          Temporary charts can be created on the ``Browse Data'' page for quick view. Select the scalars, vectors
          or histograms and choose ``Plot'' from the context menu. If you want to save such a temporary chart
          in the analysis, then choose ``Convert to dataset...'' from the context menu of the chart or
          <icon name="template.gif" />
          from the toolbar.
        </para>
      </sect3>
      <sect3>
        <title>Editing charts</title>
        <para> You can open a dialog for editing charts from the context menu. The dialog is divided into
          several pages. The pages can be opened directly from the context menu. When you select a line and
          choose ``Lines...'' from the menu, you can edit the properties of the selected line.</para>
        <para>
          You can also use the ``Properties View'' to edit the chart. It is recommended that users display the
          properties grouped according to their category (
          <icon name="Eclipse_ShowCategories.png" />
          on the toolbar of the ``Properties View'').
        </para>
        <chart-properties groups="ChartProperties">Common chart properties</chart-properties>
      </sect3>
      <sect3>
        <title>Zooming and panning</title>
        <para>
          Charts have two mouse modes. In Pan mode, you can scroll with the mouse wheel and drag the chart. In
          Zoom mode, the user can zoom in on the chart by left-clicking and zoom out by doing a
          <keycombo>
            <keycap>Shift</keycap>
            <mousebutton>left</mousebutton>
          </keycombo>
          click, or using the mouse wheel. Dragging selects a rectangular area for zooming. The toolbar icons
          <icon name="hand.gif" />
          and
          <icon name="zoom.png" />
          switch between Pan and Zoom modes. You can also find toolbar buttons to zoom in
          <icon name="zoomplus.png" />
          , zoom out (
          <icon name="zoomminus.png" />
          ) and zoom to fit (
          <icon name="zoomtofit.png" />
          ). Zooming and moving actions are remembered in the navigation history.
        </para>
      </sect3>
      <sect3>
        <title>Tooltip</title>
        <para>
          When the user hovers the mouse over a data point, the appearing tooltip shows line labels and the values
          of the points close to the cursor. The names of all lines can be displayed by hovering over the
          <icon name="legend.gif" />
          button at the top right corner of the chart.
        </para>
      </sect3>
      <sect3>
        <title>Copy to clipboard</title>
        <para> You can copy the chart to the clipboard by selecting ``Copy to Clipboard'' from the context
          menu. The chart is copied as a bitmap image and is the same size as the chart on the screen.</para>
      </sect3>
      <sect3>
        <title>Bar charts</title>
        <para> Bar charts display scalars as groups of vertical bars. The bars can be positioned within a
          group next to, above or in front of each other. The baseline of the bars can be changed. Optionally,
          a logarithmic transformation can be applied to the values.</para>
        <picture file="pictures/ANF-ScalarChart.png">Bar chart</picture>
        <para> The scalar chart's content can be specified on the ``Content'' tab of their ``Properties''
          dialog. Attributes in the "Groups" list box determine the groups so that within a group each
          attribute has the same value. Attributes in the "Bars" list box determine the bars; the bar height
          is the average of scalars that have the same values as the "Bar" attributes. You can classify the
          attributes by dragging them from the upper list boxes to the lower list boxes. You will normally
          want to group the scalars by modules and label the bars with the scalar name. This is the default
          setting, if you leave each attribute in the upper list box.</para>
        <picture file="pictures/ANF-ScalarChartEditDialog.png">Dialog page for bar chart content</picture>
        <para> In addition to the common chart properties, the properties of bar charts include:</para>
        <chart-properties groups="ScalarChartProperties BarProperties">Bar chart properties</chart-properties>
      </sect3>
      <sect3>
        <title>Line charts</title>
        <para> Line charts can be used to display output vectors. Each vector in the dataset gives a line on
          the chart. You can specify the symbols drawn at the data points (cross, diamond, dot, plus, square
          triangle or none), how the points are connected (linear, step-wise, pins or none) and the color of
          the lines. Individual lines can be hidden.</para>
        <picture file="pictures/ANF-VectorChart.png">Line chart</picture>
        <para>
          Line names identify lines on the legend, property sheets and edit dialogs. They are formed
          automatically from attributes of the vector (like file, run, module, vector name, etc.). If you want
          to name the lines yourself, you can enter a name pattern in the ``Line names'' field of the
          ``Properties'' dialog (``Main'' tab). You can use "{file}", "{run}", "{module}", or "{name}" to refer
          to an attribute value. Press
          <keycombo>
            <keycap>Ctrl</keycap>
            <keycap>Space</keycap>
          </keycombo>
          for the complete list.
        </para>
        <para> Processing operations can be applied to the dataset of the chart by selecting ``Apply'' or
          ``Compute'' from the context menu. If you want to remove an existing operation, you can do it from
          the context menu, too.</para>
        <para> Line charts are synchronized with ``Output Vector'' and ``Dataset'' views. Select a data point
          and you will see that the data point and the vector are selected in the Output Vector and ``Dataset View,'' as well.
        </para>
        <chart-properties groups="VectorChartProperties LineProperties">Line chart properties</chart-properties>
      </sect3>
      <sect3>
        <title>Histogram charts</title>
        <para>
          Histogram charts can display data of histograms. They support three view modes:
          <variablelist>
            <varlistentry>
              <term>Count</term>
              <listitem>
                <para>The chart shows the recorded counts of data points in each cell.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Probability density</term>
              <listitem>
                <para> The chart shows the probability density function computed from the histogram data.
                </para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Cumulative density</term>
              <listitem>
                <para> The chart shows the cumulative density function computed from the histogram data.
                </para>
              </listitem>
            </varlistentry>
          </variablelist>
        </para>
        <picture file="pictures/ANF-HistogramChart.png">Histogram chart</picture>
        <tip>
          <para> When drawing several histograms on one chart, set the "Bar type" property to Outline. This
            way the histograms will not cover each other.</para>
        </tip>
        <chart-properties groups="HistogramChartProperties HistogramProperties">Histogram chart properties</chart-properties>
      </sect3>
      <sect3>
        <title>Scatter charts</title>
        <para> Scatter charts can be created from both scalar and vector data. You have to select one
          statistic for the x coordinates; other data items give the y coordinates. How the x and y values are
          paired differs for scalars and vectors.</para>
        <formalpara>
          <title>Scalars</title>
          <para> For each value of the x scalar, the y values are selected from scalars in the same run.
          </para>
        </formalpara>
        <formalpara>
          <title>Vectors</title>
          <para> For each value of the x vector, the y values are selected from the same run and with the same
            simulation time.</para>
        </formalpara>
        <picture file="pictures/ANF-ScatterChart.png">A scatter chart</picture>
        <para> By default, each data point that comes from the same y scalar belongs to the same line. This is
          not always what you want because these values may have been generated in runs having different
          parameter settings; therefore, they are not homogenous. You can specify scalars to determine the "iso" lines of
          the scatter chart. Only those points that have the same values of these "iso" attributes are connected
          by lines.</para>
        <picture file="pictures/ANF-ScatterOptions.png">A scatter chart</picture>
        <tip>
          <para> If you want to use a module parameter as an iso attribute, you can record it as a scalar by
            setting "&lt;module&gt;.&lt;parameter_name&gt;.param-record-as-scalar=true"
            in the INI file.
          </para>
        </tip>
        <chart-properties groups="VectorChartProperties LineProperties">Scatter chart properties</chart-properties>
      </sect3>
    </sect2>

  </sect1>

  <sect1>
    <title>Associated Views</title>

    <sect2>
      <title>Outline View</title>
      <para>
        The ``Outline View'' shows an overview of the current analysis. Clicking on an
        element will select the corresponding element in the current editor. Tree editing
        operations also work in this view.
      </para>
      <picture file="pictures/ANF-OutlineView.png">Outline View of the analysis</picture>
    </sect2>

    <sect2>
      <title>Properties View</title>
      <para>
        The Properties View displays the properties of the selected dataset, processing node
        and chart. Font and color properties can be edited as text or by opening dialogs.
        Text fields that have a bulb on the left side have a content assist; press
        <keycombo><keycap>Ctrl</keycap><keycap>Space</keycap></keycombo> to activate it.
      </para>
      <picture file="pictures/ANF-PropertiesView.png">Properties View showing chart properties</picture>
    </sect2>

    <sect2>
      <title>Output Vector View</title>
      <para>
        The ``Output Vector View'' shows the content of the selected vector. It displays the
        serial number, simulation time and value of the data points. When event numbers
        are recorded during the simulation, they are also displayed. Large output files
        are handled efficiently; only the visible part of the vector is read from the
        disk. Vectors that are the result of a computation are saved in temporary
        files.
      </para>
      <picture file="pictures/ANF-OutputVectorView.png">Output Vector View</picture>
      <para>
        To navigate to a specific line, use the scroll bar or the menu of the view:
        <picture file="pictures/ANF-OutputVectorViewMenu.png"/>
      </para>
    </sect2>

    <sect2>
      <title>Dataset View</title>
      <para>
        The ``Dataset View'' displays the dataset's content of the selected dataset item. It is
        divided into three tables, similar to the ones on the ``Browse Data'' page. The tables can be
        selected by the
        <icon name="ShowScalars.gif"/><icon name="ShowVectors.gif"/><icon name="ShowHistograms.gif"/>
         icons. There is also a tool button (<icon name="filter.png"/>) to show or hide the filter.
      </para>
      <picture file="pictures/ANF-DatasetView.png">Dataset View</picture>
    </sect2>

  </sect1>

</chapter>
